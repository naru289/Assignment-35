{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Programme in Deep Learning (Foundations and Applications)\n",
        "## A Program by IISc and TalentSprint\n",
        "### Assignment: Implementation of Deep Convolutional Generative Adversarial Networks"
      ],
      "metadata": {
        "id": "AgYBcWnQzCNf"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r2PqtKFiBlQf"
      },
      "source": [
        "\n",
        "\n",
        "## Learning Objectives:\n",
        "\n",
        "At the end of the experiment, you will be able to :\n",
        "\n",
        "* Understand DCGAN (Deep convolutional generative adversarial networks)\n",
        "* Generate fake images of Celeba dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ksHaBQ_e8Tyr"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv-AHkmd8YCd"
      },
      "source": [
        "### Description\n",
        "\n",
        "CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes dataset with more than 200K celebrity images, each with 40 attribute annotations. The images in this dataset cover large pose variations and background clutter. CelebA has large diversities, large quantities, and rich annotations, including\n",
        "\n",
        "* 10,177 number of identities,\n",
        "\n",
        "* 202,599 number of face images, and\n",
        "\n",
        "* 5 landmark locations, 40 binary attributes annotations per image.\n",
        "\n",
        "The dataset can be employed as the training and test sets for the following computer vision tasks: face attribute recognition, face detection, landmark (or facial part) localization, and face editing & synthesis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34b8U5ov9VLz"
      },
      "source": [
        "## AI / ML Technique"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lEdzMp5f9eqo"
      },
      "source": [
        "### Generative Adversarial Networks (GANs)\n",
        "\n",
        "\n",
        "GANs are generative models devised by Goodfellow et al. in 2014. GAN is about creating, like drawing a portrait or composing a symphony. This is hard compared to other deep learning fields. For instance, it is much easier to identify a Monet painting than painting one.\n",
        "\n",
        "The main focus of GAN is to generate data from scratch, mostly images but other domains including music have been done.\n",
        "\n",
        "GAN composes of two deep networks :\n",
        "\n",
        "* Generator\n",
        "* Discriminator\n",
        "\n",
        "\n",
        "#### Generator\n",
        "\n",
        "The generator tries to produce data that come from some probability distribution. For example, that would be you trying to reproduce the party’s tickets.\n",
        "\n",
        "#### Discriminator\n",
        "\n",
        "The discriminator acts like a judge. It gets to decide if the input comes from the generator or from the true training set. For example, that would be the party’s security comparing your fake ticket with the true ticket to find flaws in your design.\n",
        "\n",
        "In summary, we can say that :\n",
        "\n",
        "\n",
        "*   The generator trying to maximize the probability of making the discriminator mistake its inputs as real.\n",
        "\n",
        "*   And the discriminator guiding the generator to produce more realistic images.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.talentsprint.com/aiml/Experiment_related_data/IMAGES/gan.png\" width=700px/>\n",
        "</center>\n",
        "\n",
        "\n",
        "### Deep Convolutional Generative Adversarial Networks (DCGANs)\n",
        "\n",
        "DCGANs are a class of convolutional GANs, where both the generator and discriminator networks are comprised of convolutional neural networks (CNNs). This means that DCGANs are perfect for all those applications which require images or videos to be fed to GANs, to generate new and plausible images and videos alike.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/DCGANS_1.png\" width=750px/>\n",
        "</center>\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What is the difference between GAN and DCGAN?\n",
        "\n",
        "DCGAN is a Deep Convolutional Generative Adversarial network that uses Deep Conv Nets to have a stable architecture and better results.\n",
        "\n",
        "\n",
        "*   The Generator in GAN uses a fully connected network, whereas DCGAN uses a Transposed Convolutional network to upsample the images.\n",
        "\n",
        "*   Both Generator and Discriminator do not use a Max pooling.\n",
        "*   Both Generator and Discriminator use Batch Normalization.\n",
        "*   The Generator uses Leaky Relu as the activation function for all layers except the output, and the Discriminator uses Leaky Relu for all layers.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "iBmefXSr0Zhu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BNLA8HiKxQhc"
      },
      "source": [
        "### Setup Steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWMVQWk58aXm"
      },
      "source": [
        "#@title Please enter your registration id to start: { run: \"auto\", display-mode: \"form\" }\n",
        "Id = \"2237180\" #@param {type:\"string\"}"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwqosl928dBA"
      },
      "source": [
        "#@title Please enter your password (normally your phone number) to continue: { run: \"auto\", display-mode: \"form\" }\n",
        "password = \"6366871391\" #@param {type:\"string\"}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "vInRrxGKIu64",
        "outputId": "4d0c4a07-7878-444f-d0ba-b45eda194d6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#@title Run this cell to complete the setup for this Notebook\n",
        "from IPython import get_ipython\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "ipython = get_ipython()\n",
        "\n",
        "notebook= \"M4_AST_35_DCGANs(Deep_Convolutional_Generative_Adversarial_Networks)_C\" #name of the notebook\n",
        "\n",
        "def setup():\n",
        "    ipython.magic(\"sx wget https://cdn.extras.talentsprint.com/DHAI/Datasets/checkpoint_20.pth.tar\")\n",
        "    from IPython.display import HTML, display\n",
        "    display(HTML('<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId={0}&recordId={1}\"></script>'.format(getId(),submission_id)))\n",
        "    print(\"Setup completed successfully\")\n",
        "    return\n",
        "\n",
        "def submit_notebook():\n",
        "    ipython.magic(\"notebook -e \"+ notebook + \".ipynb\")\n",
        "\n",
        "    import requests, json, base64, datetime\n",
        "\n",
        "    url = \"https://dashboard.talentsprint.com/xp/app/save_notebook_attempts\"\n",
        "    if not submission_id:\n",
        "      data = {\"id\" : getId(), \"notebook\" : notebook, \"mobile\" : getPassword()}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "\n",
        "      if r[\"status\"] == \"Success\":\n",
        "          return r[\"record_id\"]\n",
        "      elif \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print (\"Something is wrong, the notebook will not be submitted for grading\")\n",
        "        return None\n",
        "\n",
        "    elif getAnswer1() and getAnswer2() and getComplexity() and getAdditional() and getConcepts() and getComments() and getMentorSupport():\n",
        "      f = open(notebook + \".ipynb\", \"rb\")\n",
        "      file_hash = base64.b64encode(f.read())\n",
        "\n",
        "      data = {\"complexity\" : Complexity, \"additional\" :Additional,\n",
        "              \"concepts\" : Concepts, \"record_id\" : submission_id,\n",
        "              \"answer1\" : Answer1, \"answer2\" : Answer2, \"id\" : Id, \"file_hash\" : file_hash,\n",
        "              \"notebook\" : notebook,\n",
        "              \"feedback_experiments_input\" : Comments,\n",
        "              \"feedback_mentor_support\": Mentor_support}\n",
        "      r = requests.post(url, data = data)\n",
        "      r = json.loads(r.text)\n",
        "      if \"err\" in r:\n",
        "        print(r[\"err\"])\n",
        "        return None\n",
        "      else:\n",
        "        print(\"Your submission is successful.\")\n",
        "        print(\"Ref Id:\", submission_id)\n",
        "        print(\"Date of submission: \", r[\"date\"])\n",
        "        print(\"Time of submission: \", r[\"time\"])\n",
        "        print(\"View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\")\n",
        "        #print(\"For any queries/discrepancies, please connect with mentors through the chat icon in LMS dashboard.\")\n",
        "        return submission_id\n",
        "    else: submission_id\n",
        "\n",
        "\n",
        "def getAdditional():\n",
        "  try:\n",
        "    if not Additional:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Additional\n",
        "  except NameError:\n",
        "    print (\"Please answer Additional Question\")\n",
        "    return None\n",
        "\n",
        "def getComplexity():\n",
        "  try:\n",
        "    if not Complexity:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Complexity\n",
        "  except NameError:\n",
        "    print (\"Please answer Complexity Question\")\n",
        "    return None\n",
        "\n",
        "def getConcepts():\n",
        "  try:\n",
        "    if not Concepts:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Concepts\n",
        "  except NameError:\n",
        "    print (\"Please answer Concepts Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "# def getWalkthrough():\n",
        "#   try:\n",
        "#     if not Walkthrough:\n",
        "#       raise NameError\n",
        "#     else:\n",
        "#       return Walkthrough\n",
        "#   except NameError:\n",
        "#     print (\"Please answer Walkthrough Question\")\n",
        "#     return None\n",
        "\n",
        "def getComments():\n",
        "  try:\n",
        "    if not Comments:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Comments\n",
        "  except NameError:\n",
        "    print (\"Please answer Comments Question\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getMentorSupport():\n",
        "  try:\n",
        "    if not Mentor_support:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Mentor_support\n",
        "  except NameError:\n",
        "    print (\"Please answer Mentor support Question\")\n",
        "    return None\n",
        "\n",
        "def getAnswer1():\n",
        "  try:\n",
        "    if not Answer1:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer1\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 1\")\n",
        "    return None\n",
        "\n",
        "def getAnswer2():\n",
        "  try:\n",
        "    if not Answer2:\n",
        "      raise NameError\n",
        "    else:\n",
        "      return Answer2\n",
        "  except NameError:\n",
        "    print (\"Please answer Question 2\")\n",
        "    return None\n",
        "\n",
        "\n",
        "def getId():\n",
        "  try:\n",
        "    return Id if Id else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "def getPassword():\n",
        "  try:\n",
        "    return password if password else None\n",
        "  except NameError:\n",
        "    return None\n",
        "\n",
        "submission_id = None\n",
        "### Setup\n",
        "if getPassword() and getId():\n",
        "  submission_id = submit_notebook()\n",
        "  if submission_id:\n",
        "    setup()\n",
        "else:\n",
        "  print (\"Please complete Id and Password cells before running setup\")\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<script src=\"https://dashboard.talentsprint.com/aiml/record_ip.html?traineeId=2237180&recordId=2876\"></script>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup completed successfully\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlBDlqSH_J7s"
      },
      "source": [
        "### Importing required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M-f5ig6A3T1e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.utils import make_grid\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "import os\n",
        "import gdown\n",
        "from zipfile import ZipFile"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Downloading the dataset"
      ],
      "metadata": {
        "id": "U1cnKAJB2W-P"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bn5Z5IIyd94T"
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"celeba_gan\")\n",
        "\n",
        "# From the below drive link the dataset is getting downloaded and extracting the zip file\n",
        "url = \"https://drive.google.com/uc?id=1O7m1010EJjLE5QxLZiM9Fpjs7Oj6e684\"\n",
        "output = \"celeba_gan/data.zip\"\n",
        "gdown.download(url, output, quiet=True)\n",
        "\n",
        "with ZipFile(\"celeba_gan/data.zip\", \"r\") as zipobj:\n",
        "    zipobj.extractall(\"celeba_gan\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zKnmN4Qc_bNA"
      },
      "source": [
        "### Loading the Celeba Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DBXiMmcv8cum"
      },
      "outputs": [],
      "source": [
        "# Bacth size\n",
        "batch_size = 32\n",
        "\n",
        "stats = (0.5, 0.5, 0.5), (0.5, 0.5, 0.5)\n",
        "\n",
        "# Resize the images to 64x64 and normalize the pixel values\n",
        "transform = transforms.Compose([transforms.ToTensor(),transforms.Resize((64,64)), transforms.Normalize(*stats)])\n",
        "\n",
        "# Loading the dataset using ImageFolder\n",
        "train_dataset = datasets.ImageFolder(root = 'celeba_gan', transform = transform)\n",
        "\n",
        "# Data Loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEbfvmNiuHbp"
      },
      "outputs": [],
      "source": [
        "# Number of images in the dataset\n",
        "len(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gK3C7KS-b71"
      },
      "outputs": [],
      "source": [
        "# No of classes in the dataset\n",
        "num_classes = len(train_dataset.classes)\n",
        "print(f\"Number of classes available: {num_classes}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiFt9vxtDxAx"
      },
      "source": [
        "### Initializing the CUDA"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Every Tensor in PyTorch has a **to()** member function. Its job is to put the tensor on which it's called to a certain device whether it be the CPU or a certain GPU.\n",
        "\n",
        "Input to the to function is a torch.device object which can be initialized with either of the following inputs.\n",
        "* cpu for CPU\n",
        "* cuda:0 for putting it on GPU number 0. Similarly, if your system has multiple GPUs, then the respective number would be considered while initializing the device.\n",
        "\n",
        "Generally, whenever you initialize a Tensor, it’s put on the CPU. You should move it to the GPU to make the related calculation faster.\n"
      ],
      "metadata": {
        "id": "w3CWvMFay28Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sbX08iy_-q2P"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Visualization of CelebA dataset images"
      ],
      "metadata": {
        "id": "YviOB5k92v0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the images\n",
        "real_batch = next(iter(train_loader))\n",
        "plt.figure(figsize=(12,12))\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Training Images\")\n",
        "plt.imshow(np.transpose(vutils.make_grid(real_batch[0][:60], padding=2, normalize=True),(1,2,0)));"
      ],
      "metadata": {
        "id": "F2e8-TatSQIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRArQQGiC-fv"
      },
      "source": [
        "### Defining the Generator Model\n",
        "\n",
        "* The Generator takes the noise vector as an input to generate the images that matches the training dataset(3, 64, 64) and passed through a series of strided two-dimensional ConvTranspose2d layers.\n",
        "* It contains tranposes of CNNs and Batch Normalization layers, alternating with each other.\n",
        "* We use the ReLU activation function for all the layers except the final one, where we use the Tanh activation function (which squishes values to be between -1 and 1)\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/DCGANS_2.png\" width=700px/>\n",
        "</center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3W7SOu3t-7J-"
      },
      "outputs": [],
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "        self.gen = nn.Sequential(\n",
        "\n",
        "            nn.ConvTranspose2d(128, 512, kernel_size=4, stride=1, padding=0),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(True),\n",
        "            # out: 512 x 4 x 4\n",
        "\n",
        "            nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(True),\n",
        "            # out: 256 x 8 x 8\n",
        "\n",
        "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            # out: 128 x 16 x 16\n",
        "\n",
        "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            # out: 64 x 32 x 32\n",
        "\n",
        "            nn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1),\n",
        "            nn.Tanh()\n",
        "            # out: 3 x 64 x 64\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.gen(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXrQPlM2DFAc"
      },
      "source": [
        "### Defining the Discriminator Model\n",
        "\n",
        "* The Discriminator is a binary classification neural network to classify the input as real or fake images of dimension (3,64,64).\n",
        "* The inputs to the Discriminator are real images from the training dataset and the fake images generated by the Generator.\n",
        "* The Discriminator outputs a scalar probability to classify the input image as real or fake.\n",
        "* Input to the Discriminator is images with dimensions (3,64,64), passed through a series of Conv2d, BatchNorm2d, and LeakyReLU layers.\n",
        "* The final output layer uses Sigmoid activation function to output the scalar probability\n",
        "\n",
        "<center>\n",
        "<img src=\"https://cdn.iisc.talentsprint.com/DLFA/Experiment_related_data/DCGANS_3.png\" width=700px/>\n",
        "</center>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1VO0Ex9Z_Vzr"
      },
      "outputs": [],
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "\n",
        "        super().__init__()\n",
        "        self.dis = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # out: 64 x 32 x 32\n",
        "\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # out: 128 x 16 x 16\n",
        "\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # out: 256 x 8 x 8\n",
        "\n",
        "            nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            # out: 512 x 4 x 4\n",
        "\n",
        "            nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=0),\n",
        "            # out: 1 x 1 x 1\n",
        "\n",
        "            nn.Flatten(),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "    def forward(self, input):\n",
        "        return self.dis(input)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNjyRiLxCLHk"
      },
      "source": [
        "### Creating the Generator and Discriminator instance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r1_2ATzIfM90"
      },
      "outputs": [],
      "source": [
        "generator = Generator().to(device)\n",
        "discriminator = Discriminator().to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the generator model\n",
        "print(generator)"
      ],
      "metadata": {
        "id": "bbFjwz-q6ZwC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the discriminator model\n",
        "print(discriminator)"
      ],
      "metadata": {
        "id": "4fPl7_sx6gbc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jyv3wZbEBh-6"
      },
      "outputs": [],
      "source": [
        "# Functions to visualze the fake images from the generator as a grid\n",
        "def denorm(img_tensors):\n",
        "    return img_tensors * stats[1][0] + stats[0][0]\n",
        "\n",
        "def plot_sample(latent_tensors):\n",
        "    fake_images = denorm(generator(latent_tensors))\n",
        "    fig, ax = plt.subplots(figsize=(12, 12))\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2F_jcH_DQh1"
      },
      "source": [
        "### Training the Discriminator\n",
        "\n",
        "1. Input, the Discriminator, takes the real data from the training dataset and the fake data from the generator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbWQpZO-CgaR"
      },
      "outputs": [],
      "source": [
        "def train_discriminator(real_images, opt_d):\n",
        "    # Clear discriminator gradients\n",
        "    opt_d.zero_grad()\n",
        "\n",
        "    # Pass real images through discriminator\n",
        "    real_preds = discriminator(real_images)\n",
        "    real_targets = torch.ones(real_images.size(0), 1).to(device)\n",
        "\n",
        "    # Get the discriminator's prediction on the real image and\n",
        "    # Calculate the discriminator's loss on real images\n",
        "    real_loss = F.binary_cross_entropy(real_preds, real_targets)\n",
        "    real_score = torch.mean(real_preds).item()\n",
        "\n",
        "    # Generate fake images by passing the random noise to the generator\n",
        "    latent = torch.randn(batch_size, 128, 1, 1).to(device)\n",
        "    fake_images = generator(latent)\n",
        "\n",
        "    # Pass fake images through discriminator\n",
        "    fake_targets = torch.zeros(fake_images.size(0), 1).to(device)\n",
        "    fake_preds = discriminator(fake_images)\n",
        "    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)\n",
        "    fake_score = torch.mean(fake_preds).item()\n",
        "\n",
        "    # Update discriminator weights\n",
        "    loss = real_loss + fake_loss\n",
        "    loss.backward()\n",
        "    opt_d.step()\n",
        "    return loss.item(), real_score, fake_score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lg9J8PSgDZr2"
      },
      "source": [
        "### Training the Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4KRXT53lDMYy"
      },
      "outputs": [],
      "source": [
        "def train_generator(opt_g):\n",
        "\n",
        "    # Clear generator gradients\n",
        "    opt_g.zero_grad()\n",
        "\n",
        "    # Generate fake images\n",
        "    latent = torch.randn(batch_size, 128, 1, 1).to(device)\n",
        "    fake_images = generator(latent)\n",
        "\n",
        "    # Try to fool the discriminator\n",
        "    preds = discriminator(fake_images)\n",
        "    targets = torch.ones(batch_size, 1).to(device)\n",
        "    loss = F.binary_cross_entropy(preds, targets)\n",
        "\n",
        "    # Update generator weights\n",
        "    loss.backward()\n",
        "    opt_g.step()\n",
        "\n",
        "    return loss.item()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading a checkpoint\n",
        "\n",
        "**Note** The DCGANs model is already trained for 20 epochs and saved the checkpoint of the model and now we are loading the saved checkpoint file from that file load the generator, generator optimizer, discriminator and discriminator optimizer. The training time of DCGANs is huge, by doing this we can see how the generator model is generating the fake images which are close to our training set (instead of waiting to see the better ouput images). First we start generating the images by adding some random noise and when the model is getting trained the fake images look similar to the original images"
      ],
      "metadata": {
        "id": "8PxxOMbW7dsz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load the downloaded checkpoint file path\n",
        "def load_ckp(checkpoint_fpath, generator, opt_g, discriminator, opt_d):\n",
        "    checkpoint = torch.load(checkpoint_fpath) # Load the saved or downloaded checkpoint\n",
        "    generator.load_state_dict(checkpoint[\"gen_state\"]) # Loading the generator from the saved checkpoint\n",
        "    opt_g.load_state_dict(checkpoint[\"gen_optimizer\"]) # Loading the generator optimizer from the saved checkpoint\n",
        "    discriminator.load_state_dict(checkpoint[\"disc_state\"]) # Loading the discriminator from the saved checkpoint\n",
        "    opt_d.load_state_dict(checkpoint[\"disc_optimizer\"])  # Loading the discriminator optimizer from the saved checkpoint\n",
        "    return generator, opt_g, discriminator, opt_d"
      ],
      "metadata": {
        "id": "MdtV9f3O11Nd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above function reads the checkpoint file and loads the downloaded or saved model state and optimizer state to an instance of model and optimizer. Loading a state essentially means is that it sets the model/optimizer parameters to the values as present in the saved checkpoint."
      ],
      "metadata": {
        "id": "en9TfvxkKXu6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b3929IPDf_MD"
      },
      "source": [
        "### Lets start training the DCGAN Model\n",
        "\n",
        "**Note:** The below training process for 10 epochs will take around 1hr 30 mins to complete the execution."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lr = 0.0002\n",
        "epochs = 11\n",
        "\n",
        "# Losses & scores\n",
        "losses_g = []\n",
        "losses_d = []\n",
        "real_scores = []\n",
        "fake_scores = []\n",
        "\n",
        "# Create optimizers\n",
        "opt_d = torch.optim.Adam(discriminator.parameters(), lr=lr)\n",
        "opt_g = torch.optim.Adam(generator.parameters(), lr=lr)\n",
        "\n",
        "# You can load a model to resume training before starting the training loop.\n",
        "# Loading the checkpoint path here\n",
        "ckp_path = \"/content/checkpoint_20.pth.tar\"\n",
        "\n",
        "# Call the load checkpoint function by passing the file path\n",
        "# Basically, you first initialize your model and optimizer and then update the state dictionaries using the load checkpoint function.\n",
        "generator, opt_g, discriminator, opt_d = load_ckp(ckp_path, generator, opt_g, discriminator, opt_d)\n",
        "\n",
        "# Now you can simply pass this model and optimizer to your training loop and you would notice that the model\n",
        "# resumes training from where it left off. You can confirm this by looking at\n",
        "# the loss values after each epoch, which is in continuation of the previously observed epochs (before training stopped).\n",
        "\n",
        "for epoch in range(epochs+1):\n",
        "    for real_images, _ in train_loader:\n",
        "\n",
        "        # Train discriminator\n",
        "        real_images = real_images.to(device)\n",
        "        loss_d, real_score, fake_score = train_discriminator(real_images, opt_d)\n",
        "\n",
        "        # Train generator\n",
        "        loss_g = train_generator(opt_g)\n",
        "\n",
        "    # Record losses & scores\n",
        "    losses_g.append(loss_g)\n",
        "    losses_d.append(loss_d)\n",
        "    real_scores.append(real_score)\n",
        "    fake_scores.append(fake_score)\n",
        "\n",
        "    # Log losses & scores (last batch)\n",
        "    print(\"Epoch [{}/{}], loss_g: {:.4f}, loss_d: {:.4f}, real_score: {:.4f}, fake_score: {:.4f}\".format(\n",
        "           epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
        "\n",
        "    fixed_latent = torch.randn(64, 128, 1, 1, device=device)\n",
        "\n",
        "    # Save generated images\n",
        "    # save_samples(epoch+start_idx, fixed_latent, show=False)\n",
        "    if epoch % 10 == 0:\n",
        "        print(\"Epoch: \", epoch+1)\n",
        "        plot_sample(fixed_latent)\n",
        "\n",
        "        # You can uncomment the below lines to save your own checkpoints and see how the fake images looks like\n",
        "        # checkpoint = {\n",
        "        #              \"gen_state\": generator.state_dict(),\n",
        "        #              \"gen_optimizer\": opt_g.state_dict(),\n",
        "        #              \"disc_state\": discriminator.state_dict(),\n",
        "        #              \"disc_optimizer\": opt_d.state_dict()\n",
        "        #    }\n",
        "        # torch.save(checkpoint, f\"checkpoints/checkpoint_{epochs}.pth.tar\")"
      ],
      "metadata": {
        "id": "4-hlvnMC5yL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHfHdGCP_n6Y"
      },
      "source": [
        "### Please answer the questions below to complete the experiment:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following statements about GANs and answer question Q.1\n",
        "\n",
        "\n",
        "A. The generator takes as input a random noise and transforms it into an imitation of the real image and attempts to fool the discriminator.\n",
        "\n",
        "B. The discriminator takes as input a random noise and transforms it into an imitation of the real image and attempts to fool the generator.\n",
        "\n",
        "C. The discriminator distinguishes between the real image and the image\n",
        "created by the generator.\n",
        "\n",
        "D. The generator distinguishes between the real image and the image created by the discriminator."
      ],
      "metadata": {
        "id": "jYgyX2trhXrY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "form",
        "id": "-INhmnH1RDq0"
      },
      "outputs": [],
      "source": [
        "#@title Q.1. Which of the above statement(s) is/are True regarding Generator and Discriminator in GANs?\n",
        "Answer1 = \"Both A and C\" #@param [\"\",\"Only A\",\"Only C\", \"Only D\", \"Both A and B\", \"Both A and C\", \"Both C and D\"]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider the following statements about DCGANs and answer question Q.2\n",
        "\n",
        "A. The generator uses ReLU activation for all layers except for the output, which uses tanh activation.\n",
        "\n",
        "B. The discriminator uses LeakyReLU activation for all layers.\n",
        "\n",
        "C. Both the generator and the discriminator use Batch Normalization"
      ],
      "metadata": {
        "id": "ON8OxphJicJ1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "id": "RXPPf27bMG01"
      },
      "outputs": [],
      "source": [
        "#@title Q.2. DCGAN, is a generative adversarial network architecture. which of the above statement(s) is/are True for DCGAN, or Deep Convolutional GANs?\n",
        "Answer2 = \"A, B and C\" #@param [\"\",\"Only A\",\"Only C\", \"Only A and B\", \"Only A and C\", \"Only B and C\", \"A, B and C\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "NMzKSbLIgFzQ"
      },
      "outputs": [],
      "source": [
        "#@title How was the experiment? { run: \"auto\", form-width: \"500px\", display-mode: \"form\" }\n",
        "Complexity = \"Good and Challenging for me\" #@param [\"\",\"Too Simple, I am wasting time\", \"Good, But Not Challenging for me\", \"Good and Challenging for me\", \"Was Tough, but I did it\", \"Too Difficult for me\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DjcH1VWSFI2l"
      },
      "outputs": [],
      "source": [
        "#@title If it was too easy, what more would you have liked to be added? If it was very difficult, what would you have liked to have been removed? { run: \"auto\", display-mode: \"form\" }\n",
        "Additional = \"NA\" #@param {type:\"string\"}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4VBk_4VTAxCM"
      },
      "outputs": [],
      "source": [
        "#@title Can you identify the concepts from the lecture which this experiment covered? { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Concepts = \"Yes\" #@param [\"\",\"Yes\", \"No\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XH91cL1JWH7m"
      },
      "outputs": [],
      "source": [
        "#@title  Text and image description/explanation and code comments within the experiment: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Comments = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "z8xLqj7VWIKW"
      },
      "outputs": [],
      "source": [
        "#@title Mentor Support: { run: \"auto\", vertical-output: true, display-mode: \"form\" }\n",
        "Mentor_support = \"Somewhat Useful\" #@param [\"\",\"Very Useful\", \"Somewhat Useful\", \"Not Useful\", \"Didn't use\"]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "FzAZHt1zw-Y-",
        "outputId": "4cd6a972-a6c9-4be9-a5d3-82cc81d07837",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your submission is successful.\n",
            "Ref Id: 2876\n",
            "Date of submission:  07 Oct 2023\n",
            "Time of submission:  19:49:16\n",
            "View your submissions: https://dlfa-iisc.talentsprint.com/notebook_submissions\n"
          ]
        }
      ],
      "source": [
        "#@title Run this cell to submit your notebook for grading { vertical-output: true }\n",
        "try:\n",
        "  if submission_id:\n",
        "      return_id = submit_notebook()\n",
        "      if return_id : submission_id = return_id\n",
        "  else:\n",
        "      print(\"Please complete the setup first.\")\n",
        "except NameError:\n",
        "  print (\"Please complete the setup first.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}